---
title: "5. Уменьшение размерности: PCA"
author: "Г. Мороз"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
```

<style>
.parallax {
    /* The image used */
    background-image: url("5_PCA.jpg");

    /* Set a specific height */
    min-height: 300px; 

    /* Create the parallax scrolling effect */
    background-attachment: fixed;
    background-position: center;
    background-repeat: no-repeat;
    background-size: auto;
}
</style>

<div class="parallax"></div>

### 1. Введение
Метод главных компонент (PCA, Principal Component Analysis) применим к числовым данным, в которых строчки --- это точки наблюдения, а столбцы --- это исследуемые переменные. Данный метод часто приводят и используют как метод уменьшения размерности, однако я разделяю мнение, согласно которому это лишь метод смены перспективы, в результаты которого некоторое количество размерностей становится маловажными.

#### 1.1 Библиотеки
```{r}
library(tidyverse)
```

#### 1.2 Фамильная честь вустеров
В данной лекции я буду использовать данные из романа П. Г. Вудхауза [“Фамильная честь Вустеров”](https://en.wikipedia.org/wiki/The_Code_of_the_Woosters). В датасете собраны несколько переменных:

* chapter --- номер главы
* гарольд --- частотность появления имени в каждой из глав
* гасси --- частотность появления имени в каждой из глав
* далия --- частотность появления имени в каждой из глав
* дживс --- частотность появления имени в каждой из глав
* мадлен --- частотность появления имени в каждой из глав
* оутс --- частотность появления имени в каждой из глав
* спод --- частотность появления имени в каждой из глав
* стиффи --- частотность появления имени в каждой из глав
* сэр --- частотность появления имени в каждой из глав

```{r}
wodehouse <- read.csv("https://goo.gl/Qux1gS", sep = "\t")
library(GGally)
ggpairs(wodehouse[,-1])+
  theme_bw()
```



<div class="parallax"></div>

### 2. Дисперсия, ковариация, корреляция Пирсона

$$var(X) = \frac{\sum_{i = 1}^n(x_i - \bar{x})^2}{n - 1}$$

$$cov(X, Y) = \frac{\sum_{i = 1}^n(x_i - \bar{x})(y_i-\bar{y})}{n - 1}$$

```{r}
cov(wodehouse[,-1])
```

$$cor(X, Y) = \frac{cov(X, Y)}{\sigma_X\times\sigma_Y}$$

```{r}
cor(wodehouse[,-1])
```

<div class="parallax"></div>

### 3. Собственный вектор, собственное значение

Как вы знаете матрицы можно перемножать. [Подсказка](https://upload.wikimedia.org/wikipedia/commons/e/eb/Matrix_multiplication_diagram_2.svg).

```{r}
m1 <- matrix(c(2, 2, 3, 1), nrow = 2)
m2 <- matrix(c(1, 3), nrow = 2)
m3 <- matrix(c(3, 2), nrow = 2)
m1
m2
m1 %*% m2
m1
m3
m1 %*% m3
```

В первом примере мы получили матрицу $\left(\array{11\\ 5}\right)$, а во втором случае $\left(\array{12\\ 8}\right) = 4 \times \left(\array{3\\ 2}\right)$, т. е. при умножении матрицы мы получили значение, равное скалярному умножению той же самой матрицы.

Мы можем думать об одной матрице, как о векторе $\left(\array{3\\ 2}\right)$ в двумерном пространстве. Тогда матрица $\left(\array{2 & 3\\ 2 & 1}\right)$ --- это матрица некоторой трансформации А, которая изменяет вектор $\left(\array{3\\ 2}\right)$. В таком случае **собственный вектор (eigenvector)** --- это тот постянный объект, который подвергается трансформации, а **собственное значение (eigenvalues)** --- это скалярный мультипликатор собсвтенного вектора (в нашем случае собственное значение равно 4).

Свойства собсвтенных векторов:

* собственные векторы можно найти только для квадратных матриц (и то не для всех)
* все собственные векторы матрицы перпендекулярны друг другу вне зависимости от размерности.
* принято задавать собственные векторы длинной 1, так что найдя собственный вектор $\left(\array{3\\ 2}\right)$, мы узнаем его длинну $$\sqrt{3^2+2^2} = \sqrt{13},$$ так что теперь можно отмасштабировать вектор: $$\left(\array{3\\ 2}\right) \times \frac{1}{\sqrt{13}} = \left(\array{3/\sqrt{13}\\ 2/\sqrt{13}}\right)$$

Как найти собственный вектор в R:

```{r}
m <- matrix(c(2, 2, 3, 1), nrow = 2)
eigen(m)
```

Собственные значения в переменной `values` функция всегда возвращает в убывающем порядке, а каждая колонка в переменной `eigenvectors` соответствует элементу в переменной `values`. Сравните со значениями, которые мы получили руками:

```{r}
3/sqrt(13)
2/sqrt(13)
```

<div class="parallax"></div>

### 4. PCA
Обычно переменные, которые используют в PCA **нужно обязательно нормализовать**, но так как мы будем использовать частотность, эти переменные не нуждаются в нормализации. Давайте сравним результат работы функций, которые мы рассмотрели перед этим и функции `prcomp`.

```{r}
eigen(cov(wodehouse[,-1]))
PCA <- prcomp(wodehouse[,-1])
PCA
```

Как читать полученное?  Мы сменили оси координат и в новом пространстве (точно так же 9-мерном) мы можем перейти используя полученные значения:

$$PC1 = гарольд \times 0.03548428 + гасси \times 0.08477226 + далия \times -0.11013760 + дживс \times -0.48849572 +$$ 
$$ + мадлен \times 0.12377778 + оутс \times -0.04712363 + спод \times 0.09814424 + стиффи \times 0.05838698 + сэр \times -0.84274152$$

Как полученные компоненты объясняют дисперсию в переменных?

```{r}
summary(PCA)
```

Т. е. первые две компоненты объясняют почти 80 процентов дисперсии, это достаточно высокое значение, которое позволяет нам применять данный метод.

Дальнейшая визуализация возможна благодаря пакету `ggfortify`:

```{r}
library(ggfortify)
autoplot(PCA,
         shape = FALSE,
         loadings = TRUE,
         label = TRUE,
         loadings.label = TRUE)+
  theme_bw()
```

Числа на этом графике --- номера глав романа, красные линии --- оси старых осей координат. Сам график называется биплот. Чем ближе друг к другустарые оси координат, тем больше скоррелированы переменные (вообще, косинус угла между ними равен коэфициенту корреляции между соответствующими переменными).

<div class="parallax"></div>

### 5. Что дальше?

* После того как преобразование сделано можно запскать стандартные методы регрессии, кластеризации и т. д.
* можно использовать некоторые не затронутые в PCA переменные, для анализа в новом пространстве

<div class="parallax"></div>

### 6. 3d пример от Ильи Щурова
[ссылка](http://math-info.hse.ru/f/2015-16/ling-mag-quant/lecture-pca.html#%D0%A2%D1%80%D1%91%D1%85%D0%BC%D0%B5%D1%80%D0%BD%D1%8B%D0%B9%20%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80)

<div class="parallax"></div>

### 7. Евангелия
```{r}
gospels <- read.csv("https://goo.gl/mdBVVe")
```

<div class="parallax"></div>